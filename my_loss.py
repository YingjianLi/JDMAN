import torchimport torch.nn as nnfrom torch.autograd.function import Functionimport numpy as npclass MC_Loss_dis_1_center(nn.Module):    def __init__(self, num_classes=7, feat_dim=2048, lamda1=1.0, size_average=True):        super(MC_Loss_dis_1_center, self).__init__()        self.centers_source = nn.Parameter(torch.randn(num_classes, feat_dim))        self.centers_target = nn.Parameter(torch.randn(num_classes, feat_dim))        self.feat_dim = feat_dim        self.size_average = size_average        self.num_classes = num_classes        self.lamda1 = lamda1        self.M = 100    def forward(self, feat,label):        feature_source = feat[:feat.size(0)//2]        feature_target = feat[feat.size(0)//2:]        label_source = label[:label.size(0)//2]        label_target= label[label.size(0)//2:]        batch_size = feat.size(0)//2        if feat.size(1) != self.feat_dim:            raise ValueError("Center's dim: {0} should be equal to input feature's \                                    dim: {1}".format(self.feat_dim, feat.size(1)))        batch_size_tensor = feat.new_empty(1).fill_(            batch_size if self.size_average else 1)  # 存放batch的大小，用于控制输出是平均loss还是总loss        source_centers_batch = self.centers_source.index_select(0, label_source.long())  # 选出这个batch有关的center（一个batch不一定包含所有类别的样本        target_centers_batch = self.centers_target.index_select(0, label_target.long())        # intra class loss        source_center_loss = (feature_source - source_centers_batch).pow(2).sum() / 2.0 / batch_size_tensor  # 只计算这个batch中有样本的类别的loss，更新时也只更新他们        target_center_loss = (feature_target - target_centers_batch).pow(2).sum() / 2.0 / batch_size_tensor        # inter class loss        source_inter_class_loss = torch.cuda.FloatTensor([0.])        target_inter_class_loss = torch.cuda.FloatTensor([0.])        for i in range(self.num_classes):            for j in range(i + 1, self.num_classes):                source_inter_class_loss += (self.centers_source[i] - self.centers_source[j]).pow(2).sum()/self.num_classes/(self.num_classes-1)  # all loss, not the average of batch        for i in range(self.num_classes):            for j in range(i + 1, self.num_classes):                target_inter_class_loss += (self.centers_target[i] - self.centers_target[j]).pow(2).sum()/self.num_classes/(self.num_classes-1)  # all loss, not the average of batch        # domain center loss        domain_center_loss = torch.cuda.FloatTensor([0.])        for i in range(self.num_classes):            domain_center_loss += (self.centers_source[i] - self.centers_target[i]).pow(2).sum()/self.num_classes        loss = source_center_loss + target_center_loss+\               max(self.M - source_inter_class_loss, torch.cuda.FloatTensor([0.]))+ \               max(self.M - target_inter_class_loss, torch.cuda.FloatTensor([0.]))+ \               self.lamda1 * domain_center_loss        return lossdef Entropy(input_):    epsilon = 1e-5    entropy = -input_ * torch.log(input_ + epsilon)    entropy = torch.sum(entropy, dim=1)    return entropydef grl_hook(coeff): # GRL layer    def fun1(grad):        return -coeff * grad.clone()    return fun1# calculte the adv loss by CDANdef CDAN(input_list, ad_net, entropy=None, coeff=None, random_layer=None):    softmax_output = input_list[1].detach()    feature = input_list[0]    if random_layer is None:        # op_out = torch.bmm(softmax_output.unsqueeze(2), feature.unsqueeze(1))        ad_out = ad_net(feature.view(-1, feature.size(1)))    else:        random_out = random_layer.forward([feature, softmax_output])        ad_out = ad_net(random_out.view(-1, random_out.size(1)))    batch_size = softmax_output.size(0) // 2    dc_target = torch.from_numpy(np.array([[1]] * batch_size + [[0]] * batch_size)).float().cuda()    if entropy is not None:        entropy.register_hook(grl_hook(coeff))        entropy = 1.0 + torch.exp(-entropy)        source_mask = torch.ones_like(entropy)        source_mask[feature.size(0) // 2:] = 0        source_weight = entropy * source_mask # entropy weight        target_mask = torch.ones_like(entropy)        target_mask[0:feature.size(0) // 2] = 0        target_weight = entropy * target_mask        weight = source_weight / torch.sum(source_weight).detach().item() + \                 target_weight / torch.sum(target_weight).detach().item()        #l = nn.BCELoss(reduction='none')(ad_out, dc_target)        # print(len(ad_out),len(dc_target))        return torch.sum(weight.view(-1, 1) * nn.BCELoss()(ad_out, dc_target)) / torch.sum(weight).detach().item()        # return weighted domain adv loss, with GRL layer    else:        return nn.BCELoss()(ad_out, dc_target)